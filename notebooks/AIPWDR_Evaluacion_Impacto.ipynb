{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUACIÓN DE IMPACTO DEL PROGRAMA PTA/FI 3.0\n",
    "## Metodología AIPW/DR con Wild Cluster Bootstrap\n",
    "\n",
    "---\n",
    "\n",
    "### Información del Análisis\n",
    "- **Fecha de análisis**: Diciembre 2025\n",
    "- **Metodología**: Augmented Inverse Propensity Weighting / Doubly Robust (AIPW/DR)\n",
    "- **Inferencia**: Wild Cluster Bootstrap con pesos Rademacher\n",
    "- **Unidad de clustering**: Escuela (código DANE)\n",
    "- **Outcomes**: EGRA (lectura) y EGMA (matemáticas)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MARCO METODOLÓGICO\n",
    "\n",
    "## 1.1 Fundamentos del Estimador AIPW/DR\n",
    "\n",
    "El estimador **Augmented Inverse Propensity Weighting (AIPW)**, también conocido como **Doubly Robust (DR)**, combina dos enfoques fundamentales de inferencia causal:\n",
    "\n",
    "### Componente 1: Inverse Propensity Weighting (IPW)\n",
    "Pondera las observaciones por el inverso de la probabilidad de recibir el tratamiento:\n",
    "\n",
    "$$\\hat{\\tau}_{IPW} = \\frac{1}{n}\\sum_{i=1}^{n}\\left[\\frac{D_i Y_i}{e(X_i)} - \\frac{(1-D_i)Y_i}{1-e(X_i)}\\right]$$\n",
    "\n",
    "### Componente 2: Regression Adjustment (RA)\n",
    "Modela directamente los resultados potenciales:\n",
    "\n",
    "$$\\hat{\\tau}_{RA} = \\frac{1}{n}\\sum_{i=1}^{n}\\left[\\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i)\\right]$$\n",
    "\n",
    "### Estimador AIPW/DR Combinado\n",
    "\n",
    "$$\\hat{\\tau}_{AIPW} = \\frac{1}{n}\\sum_{i=1}^{n}\\left[\\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i(Y_i - \\hat{\\mu}_1(X_i))}{e(X_i)} - \\frac{(1-D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1-e(X_i)}\\right]$$\n",
    "\n",
    "Donde:\n",
    "- $D_i$: Indicador de tratamiento (1=tratado, 0=control)\n",
    "- $Y_i$: Resultado observado (puntaje EGRA/EGMA)\n",
    "- $e(X_i)$: Propensity Score (probabilidad de tratamiento)\n",
    "- $\\hat{\\mu}_1(X_i)$, $\\hat{\\mu}_0(X_i)$: Resultados esperados predichos\n",
    "\n",
    "## 1.2 Propiedad de Doble Robustez\n",
    "\n",
    "El estimador AIPW es **doblemente robusto**: produce estimaciones consistentes si:\n",
    "1. El modelo de propensión $e(X)$ está correctamente especificado, **O**\n",
    "2. Los modelos de resultado $\\mu_1(X)$, $\\mu_0(X)$ están correctamente especificados\n",
    "\n",
    "## 1.3 Wild Cluster Bootstrap\n",
    "\n",
    "Dado que el tratamiento se asigna a nivel de **escuela** (cluster), usamos **Wild Cluster Bootstrap** con pesos Rademacher:\n",
    "\n",
    "1. Para cada iteración bootstrap $b = 1, ..., B$:\n",
    "   - Generar pesos Rademacher $w_s \\in \\{-1, +1\\}$ para cada escuela $s$\n",
    "   - Calcular efectos individuales perturbados\n",
    "   - Obtener ATE bootstrap\n",
    "\n",
    "2. El error estándar es la desviación estándar de la distribución bootstrap\n",
    "\n",
    "3. Los intervalos de confianza se obtienen de los percentiles 2.5 y 97.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CONFIGURACIÓN DEL ENTORNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTACIÓN DE LIBRERÍAS\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración global\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUACIÓN DE IMPACTO PTA/FI 3.0\")\n",
    "print(\"Metodología AIPW/DR con Wild Cluster Bootstrap\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PARÁMETROS DE CONFIGURACIÓN\n",
    "# ==============================================================================\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\Jhon Narvaez\\Downloads\")\n",
    "OUTPUT_DIR = BASE_DIR / \"EVALUACION_IMPACTO_FINAL\" / \"Resultados\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "CONFIG = {\n",
    "    'AUROC_MIN': 0.6,\n",
    "    'AUROC_MAX': 0.9,\n",
    "    'SMD_MAX': 0.10,\n",
    "    'CLIP_MIN': 0.01,\n",
    "    'CLIP_MAX': 0.99,\n",
    "    'N_BOOTSTRAP': 1000,\n",
    "    'N_FOLDS': 5,\n",
    "    'RANDOM_STATE': 42\n",
    "}\n",
    "\n",
    "print(\"\\nCONFIGURACIÓN DEL ANÁLISIS:\")\n",
    "print(\"-\" * 40)\n",
    "for param, valor in CONFIG.items():\n",
    "    print(f\"  {param}: {valor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CARGA DE LA BASE DE DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "ruta_estudiantes = BASE_DIR / \"BASES_DEFINITIVAS_AIPW\" / \"base_estudiante_FINAL.csv\"\n",
    "df_original = pd.read_csv(ruta_estudiantes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASE DE DATOS CARGADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDimensiones:\")\n",
    "print(f\"  - Total registros: {len(df_original):,}\")\n",
    "print(f\"  - Total variables: {len(df_original.columns)}\")\n",
    "print(f\"\\nEstructura de clusters:\")\n",
    "print(f\"  - Escuelas únicas: {df_original['codigo_dane'].nunique()}\")\n",
    "print(f\"  - Estudiantes tratamiento: {df_original['tratamiento'].sum()}\")\n",
    "print(f\"  - Estudiantes control: {(1-df_original['tratamiento']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZACIÓN DE LA BASE ORIGINAL (PRE-ETL)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZACIÓN BASE ORIGINAL (ANTES DE ETL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPrimeras 10 filas:\")\n",
    "display(df_original.head(10))\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "display(df_original.describe())\n",
    "\n",
    "print(\"\\nValores faltantes por variable:\")\n",
    "missing = df_original.isnull().sum()\n",
    "missing_pct = (missing / len(df_original) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Faltantes': missing, 'Porcentaje': missing_pct})\n",
    "display(missing_df[missing_df['Faltantes'] > 0].sort_values('Porcentaje', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# EXPORTAR BASE DE DATOS PRE-ETL (ORIGINAL)\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORTANDO BASE DE DATOS PRE-ETL\")\nprint(\"=\"*80)\n\n# Crear carpeta para bases de datos\nDATOS_DIR = BASE_DIR / \"EVALUACION_IMPACTO_FINAL\" / \"Bases_Datos\"\nDATOS_DIR.mkdir(exist_ok=True, parents=True)\n\n# Exportar base original\ndf_original.to_csv(DATOS_DIR / 'BASE_PRE_ETL.csv', index=False, encoding='utf-8-sig')\ndf_original.to_excel(DATOS_DIR / 'BASE_PRE_ETL.xlsx', index=False)\n\nprint(f\"\\n[OK] Bases PRE-ETL exportadas:\")\nprint(f\"  - {DATOS_DIR / 'BASE_PRE_ETL.csv'}\")\nprint(f\"  - {DATOS_DIR / 'BASE_PRE_ETL.xlsx'}\")\nprint(f\"\\nDimensiones: {df_original.shape[0]} filas x {df_original.shape[1]} columnas\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PROCESO ETL (Extract, Transform, Load)\n",
    "\n",
    "## 4.1 Construcción de Variables Outcome\n",
    "\n",
    "### EGRA Total (Lectura)\n",
    "Promedio de 4 subpruebas: CSL, DPSS, LP, CL\n",
    "\n",
    "### EGMA Total (Matemáticas)\n",
    "Promedio de 6 subpruebas: MissNum, CompMatOp, Sumas, Restas, MultDiv, Problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONSTRUCCIÓN DE VARIABLES OUTCOME\n",
    "# ==============================================================================\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "EGRA_vars = ['CSL_pct_correctas', 'DPSS_pct_correctas',\n",
    "             'LP_pct_correctas', 'CL_pct_correctas']\n",
    "\n",
    "EGMA_vars = ['miss_num_pct_correctas', 'comp_mat_op_pct_correctas',\n",
    "             'sums_pct_correctas', 'substract_pct_correctas',\n",
    "             'mult_div_pct_correctas', 'res_problems_pct_correctas']\n",
    "\n",
    "df['EGRA_Total'] = df[EGRA_vars].mean(axis=1)\n",
    "df['EGMA_Total'] = df[EGMA_vars].mean(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONSTRUCCIÓN DE OUTCOMES\")\n",
    "print(\"=\"*80)\n",
    "for outcome in ['EGRA_Total', 'EGMA_Total']:\n",
    "    print(f\"\\n{outcome}:\")\n",
    "    print(f\"  Media: {df[outcome].mean():.2f}\")\n",
    "    print(f\"  Desv. Est.: {df[outcome].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tratamiento de Valores Faltantes\n",
    "\n",
    "| Variable | % Faltantes | Decisión |\n",
    "|----------|-------------|----------|\n",
    "| tei_slope_pre | ~40% | NO imputar - crear indicador binario |\n",
    "| tei_nivel_pre | <5% | Imputar mediana |\n",
    "| ruralidad | <5% | Imputar mediana |\n",
    "| pdet | <5% | Imputar mediana |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRATAMIENTO DE VALORES FALTANTES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TEI Slope - NO imputar, crear indicador\n",
    "df['tiene_tei_slope'] = df['tei_slope_pre'].notna().astype(int)\n",
    "print(f\"\\n1. TEI Slope: Indicador binario creado\")\n",
    "print(f\"   Con datos: {df['tiene_tei_slope'].sum()} ({df['tiene_tei_slope'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Variables numéricas\n",
    "print(\"\\n2. Variables numéricas (imputación con mediana):\")\n",
    "vars_imputar = ['tei_nivel_pre', 'ruralidad', 'pdet', 'matricula_pre', \n",
    "                'pct_jornada_completa', 'composicion_grados', 'num_jornadas']\n",
    "\n",
    "for var in vars_imputar:\n",
    "    if var in df.columns and df[var].isna().any():\n",
    "        n_na = df[var].isna().sum()\n",
    "        mediana = df[var].median()\n",
    "        df[var] = df[var].fillna(mediana)\n",
    "        print(f\"   {var}: {n_na} imputados con mediana = {mediana:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Creación de Variables Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CREACIÓN DE VARIABLES DUMMY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREACIÓN DE VARIABLES DUMMY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'zona_ptafi' in df.columns:\n",
    "    moda_zona = df['zona_ptafi'].mode()[0] if not df['zona_ptafi'].mode().empty else 'C'\n",
    "    df['zona_ptafi'] = df['zona_ptafi'].fillna(moda_zona)\n",
    "    \n",
    "    zona_dummies = pd.get_dummies(df['zona_ptafi'], prefix='zona', drop_first=True)\n",
    "    df = pd.concat([df, zona_dummies], axis=1)\n",
    "    \n",
    "    print(f\"\\nDummies de zona creadas: {list(zona_dummies.columns)}\")\n",
    "    for col in zona_dummies.columns:\n",
    "        print(f\"  {col}: {df[col].sum()} ({df[col].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Definición de Covariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DEFINICIÓN DE COVARIABLES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINICIÓN DE COVARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "zona_cols = [c for c in df.columns if c.startswith('zona_') and c != 'zona_ptafi']\n",
    "\n",
    "COVARIABLES = [\n",
    "    'edad', 'sexo',\n",
    "    'ruralidad', 'pdet',\n",
    "    'tei_nivel_pre',\n",
    "    'matricula_pre',\n",
    "    'pct_jornada_completa',\n",
    "    'composicion_grados',\n",
    "] + zona_cols\n",
    "\n",
    "COVARIABLES = [c for c in COVARIABLES if c in df.columns]\n",
    "\n",
    "print(f\"\\nCovariables seleccionadas ({len(COVARIABLES)}):\")\n",
    "for cov in COVARIABLES:\n",
    "    print(f\"  - {cov}: NaN={df[cov].isna().sum()}\")\n",
    "\n",
    "print(\"\\nVariables EXCLUIDAS (alto SMD):\")\n",
    "print(\"  - tiene_tei_slope: SMD alto\")\n",
    "print(\"  - num_jornadas: SMD alto\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# EXPORTAR BASE DE DATOS POST-ETL (PROCESADA)\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORTANDO BASE DE DATOS POST-ETL\")\nprint(\"=\"*80)\n\n# Seleccionar todas las variables relevantes para exportar\ncols_exportar = ['codigo_dane', 'tratamiento', 'edad', 'sexo']\n\n# Agregar outcomes\ncols_exportar += ['EGRA_Total', 'EGMA_Total']\n\n# Agregar subpruebas EGRA\ncols_exportar += EGRA_vars\n\n# Agregar subpruebas EGMA\ncols_exportar += EGMA_vars\n\n# Agregar covariables\ncols_exportar += COVARIABLES\n\n# Agregar variables adicionales\ncols_adicionales = ['tiene_tei_slope', 'tei_slope_pre', 'zona_ptafi']\ncols_exportar += [c for c in cols_adicionales if c in df.columns]\n\n# Agregar dummies de zona\ncols_exportar += [c for c in df.columns if c.startswith('zona_') and c not in cols_exportar]\n\n# Eliminar duplicados manteniendo orden\ncols_exportar = list(dict.fromkeys(cols_exportar))\ncols_exportar = [c for c in cols_exportar if c in df.columns]\n\n# Crear DataFrame de exportación\ndf_post_etl = df[cols_exportar].copy()\n\n# Exportar\ndf_post_etl.to_csv(DATOS_DIR / 'BASE_POST_ETL.csv', index=False, encoding='utf-8-sig')\ndf_post_etl.to_excel(DATOS_DIR / 'BASE_POST_ETL.xlsx', index=False)\n\nprint(f\"\\n[OK] Bases POST-ETL exportadas:\")\nprint(f\"  - {DATOS_DIR / 'BASE_POST_ETL.csv'}\")\nprint(f\"  - {DATOS_DIR / 'BASE_POST_ETL.xlsx'}\")\nprint(f\"\\nDimensiones: {df_post_etl.shape[0]} filas x {df_post_etl.shape[1]} columnas\")\nprint(f\"\\nVariables incluidas ({len(cols_exportar)}):\")\nfor i, col in enumerate(cols_exportar, 1):\n    print(f\"  {i:2d}. {col}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZACIÓN DE LA BASE PROCESADA (POST-ETL)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZACIÓN BASE PROCESADA (DESPUÉS DE ETL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cols_mostrar = ['codigo_dane', 'tratamiento', 'edad', 'sexo', 'EGRA_Total', 'EGMA_Total']\n",
    "cols_mostrar = [c for c in cols_mostrar if c in df.columns]\n",
    "\n",
    "print(\"\\nPrimeras 10 filas (variables principales):\")\n",
    "display(df[cols_mostrar].head(10))\n",
    "\n",
    "print(\"\\nResumen de covariables:\")\n",
    "display(df[COVARIABLES].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ANÁLISIS DE VARIANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ANÁLISIS DE VARIANZA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DE VARIANZA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calcular_icc(df, outcome, grupo_var='codigo_dane'):\n",
    "    grupos = df.groupby(grupo_var)[outcome]\n",
    "    medias_grupo = grupos.mean()\n",
    "    media_global = df[outcome].mean()\n",
    "    n_por_grupo = grupos.size()\n",
    "    var_between = np.sum(n_por_grupo * (medias_grupo - media_global)**2) / (len(df) - 1)\n",
    "    var_within = grupos.var().mean()\n",
    "    icc = var_between / (var_between + var_within) if (var_between + var_within) > 0 else 0\n",
    "    return icc\n",
    "\n",
    "print(\"\\n1. CORRELACIÓN INTRACLASE (ICC):\")\n",
    "for outcome in ['EGRA_Total', 'EGMA_Total']:\n",
    "    icc = calcular_icc(df, outcome)\n",
    "    print(f\"   {outcome}: ICC = {icc:.4f} ({icc*100:.1f}% varianza entre escuelas)\")\n",
    "\n",
    "print(\"\\n2. TEST DE LEVENE:\")\n",
    "for outcome in ['EGRA_Total', 'EGMA_Total']:\n",
    "    trat = df[df['tratamiento'] == 1][outcome].dropna()\n",
    "    ctrl = df[df['tratamiento'] == 0][outcome].dropna()\n",
    "    stat, p = stats.levene(trat, ctrl)\n",
    "    print(f\"   {outcome}: W={stat:.3f}, p={p:.4f}\")\n",
    "\n",
    "print(\"\\n3. ANOVA ENTRE ESCUELAS:\")\n",
    "for outcome in ['EGRA_Total', 'EGMA_Total']:\n",
    "    grupos_data = [g[outcome].dropna().values for _, g in df.groupby('codigo_dane')]\n",
    "    grupos_data = [g for g in grupos_data if len(g) > 0]\n",
    "    f_stat, p_val = stats.f_oneway(*grupos_data)\n",
    "    print(f\"   {outcome}: F={f_stat:.3f}, p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ==============================================================================\n",
    "\n",
    "def calcular_smd(df, var, tratamiento_col='tratamiento'):\n",
    "    \"\"\"Calcula el Standardized Mean Difference (SMD)\"\"\"\n",
    "    trat = df[df[tratamiento_col] == 1][var].dropna()\n",
    "    ctrl = df[df[tratamiento_col] == 0][var].dropna()\n",
    "    if len(trat) == 0 or len(ctrl) == 0:\n",
    "        return 0\n",
    "    var_pooled = np.sqrt((trat.var() + ctrl.var()) / 2)\n",
    "    if var_pooled == 0:\n",
    "        return 0\n",
    "    return abs((trat.mean() - ctrl.mean()) / var_pooled)\n",
    "\n",
    "def calcular_ess(weights):\n",
    "    \"\"\"Calcula el Effective Sample Size (ESS)\"\"\"\n",
    "    return (np.sum(weights) ** 2) / np.sum(weights ** 2)\n",
    "\n",
    "def smd_ponderado(df, var, weights_norm, y):\n",
    "    \"\"\"Calcula el SMD después de aplicar pesos IPW\"\"\"\n",
    "    mask_t = y == 1\n",
    "    mask_c = y == 0\n",
    "    mean_t = np.average(df[var].values[mask_t], weights=weights_norm[mask_t])\n",
    "    mean_c = np.average(df[var].values[mask_c], weights=weights_norm[mask_c])\n",
    "    var_t = np.average((df[var].values[mask_t] - mean_t)**2, weights=weights_norm[mask_t])\n",
    "    var_c = np.average((df[var].values[mask_c] - mean_c)**2, weights=weights_norm[mask_c])\n",
    "    var_pooled = np.sqrt((var_t + var_c) / 2)\n",
    "    return abs((mean_t - mean_c) / var_pooled) if var_pooled > 0 else 0\n",
    "\n",
    "print(\"Funciones auxiliares definidas: calcular_smd, calcular_ess, smd_ponderado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MODELO DE PROPENSIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODELO DE PROPENSIÓN (Logistic Ridge)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO DE PROPENSIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = df[COVARIABLES].values\n",
    "y = df['tratamiento'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nEntrenando Logistic Ridge...\")\n",
    "model = LogisticRegressionCV(\n",
    "    cv=5, penalty='l2', solver='lbfgs',\n",
    "    Cs=np.logspace(-4, 2, 20), max_iter=2000,\n",
    "    random_state=CONFIG['RANDOM_STATE']\n",
    ")\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "ps = model.predict_proba(X_scaled)[:, 1]\n",
    "ps = np.clip(ps, CONFIG['CLIP_MIN'], CONFIG['CLIP_MAX'])\n",
    "\n",
    "df['propensity_score'] = ps\n",
    "\n",
    "print(f\"\\nC óptimo: {model.C_[0]:.4f}\")\n",
    "print(f\"\\nDistribución de Propensity Scores:\")\n",
    "print(f\"  Min: {ps.min():.4f}\")\n",
    "print(f\"  Mediana: {np.median(ps):.4f}\")\n",
    "print(f\"  Max: {ps.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. DIAGNÓSTICOS DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DIAGNÓSTICOS: AUROC Y ESS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNÓSTICOS DEL MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "auroc = roc_auc_score(y, ps)\n",
    "print(f\"\\n1. AUROC = {auroc:.4f}\")\n",
    "if 0.6 <= auroc <= 0.9:\n",
    "    print(f\"   [OK] En rango óptimo [0.6, 0.9]\")\n",
    "\n",
    "# ESS\n",
    "weights_t = 1 / ps[y == 1]\n",
    "weights_c = 1 / (1 - ps[y == 0])\n",
    "weights_t_norm = weights_t / weights_t.sum() * len(weights_t)\n",
    "weights_c_norm = weights_c / weights_c.sum() * len(weights_c)\n",
    "\n",
    "ess_t = calcular_ess(weights_t_norm)\n",
    "ess_c = calcular_ess(weights_c_norm)\n",
    "n_tratados = (y == 1).sum()\n",
    "n_control = (y == 0).sum()\n",
    "\n",
    "print(f\"\\n2. EFFECTIVE SAMPLE SIZE (ESS):\")\n",
    "print(f\"   Tratamiento: {ess_t:.0f}/{n_tratados} ({ess_t/n_tratados*100:.1f}%)\")\n",
    "print(f\"   Control: {ess_c:.0f}/{n_control} ({ess_c/n_control*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BALANCE DE COVARIABLES (SMD)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCE DE COVARIABLES (SMD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SMD SIN PONDERAR\n",
    "print(f\"\\n3. SMD SIN PONDERAR:\")\n",
    "smds_crudo = {}\n",
    "for var in COVARIABLES:\n",
    "    smd = calcular_smd(df, var)\n",
    "    smds_crudo[var] = smd\n",
    "    estado = \"[OK]\" if smd < 0.10 else \"[!]\" if smd < 0.25 else \"[X]\"\n",
    "    print(f\"   {var:30s}: {smd:.4f} {estado}\")\n",
    "\n",
    "smd_promedio_crudo = np.mean(list(smds_crudo.values()))\n",
    "print(f\"\\n   SMD Promedio: {smd_promedio_crudo:.4f}\")\n",
    "\n",
    "# SMD PONDERADO\n",
    "print(f\"\\n4. SMD PONDERADO (IPW):\")\n",
    "weights_ipw = np.where(y == 1, 1/ps, 1/(1-ps))\n",
    "weights_norm = weights_ipw.copy()\n",
    "weights_norm[y==1] = weights_ipw[y==1] / weights_ipw[y==1].sum() * (y==1).sum()\n",
    "weights_norm[y==0] = weights_ipw[y==0] / weights_ipw[y==0].sum() * (y==0).sum()\n",
    "\n",
    "smds_ipw = {}\n",
    "for var in COVARIABLES:\n",
    "    smd_w = smd_ponderado(df, var, weights_norm, y)\n",
    "    smds_ipw[var] = smd_w\n",
    "    mejora = ((smds_crudo[var] - smd_w) / smds_crudo[var] * 100) if smds_crudo[var] > 0 else 0\n",
    "    estado = \"[OK]\" if smd_w < 0.10 else \"[!]\"\n",
    "    print(f\"   {var:30s}: {smd_w:.4f} {estado} (mejora: {mejora:+.1f}%)\")\n",
    "\n",
    "smd_promedio_ipw = np.mean(list(smds_ipw.values()))\n",
    "print(f\"\\n   SMD Promedio (IPW): {smd_promedio_ipw:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. LOVE PLOT - VISUALIZACIÓN DEL BALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOVE PLOT\n",
    "# ==============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# SMD Crudo\n",
    "ax1 = axes[0]\n",
    "smds_sorted = sorted(smds_crudo.items(), key=lambda x: x[1], reverse=True)\n",
    "vars_sorted = [x[0] for x in smds_sorted]\n",
    "vals_sorted = [x[1] for x in smds_sorted]\n",
    "colors = ['red' if v > 0.25 else 'orange' if v > 0.10 else 'green' for v in vals_sorted]\n",
    "\n",
    "ax1.barh(vars_sorted, vals_sorted, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(0.10, color='green', linestyle='--', linewidth=2, label='Meta SMD < 0.10')\n",
    "ax1.axvline(0.25, color='red', linestyle='--', linewidth=2, label='Umbral SMD = 0.25')\n",
    "ax1.set_xlabel('SMD')\n",
    "ax1.set_title('Love Plot - SMD SIN PONDERAR', fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# SMD Ponderado\n",
    "ax2 = axes[1]\n",
    "smds_ipw_sorted = sorted(smds_ipw.items(), key=lambda x: x[1], reverse=True)\n",
    "vars_ipw = [x[0] for x in smds_ipw_sorted]\n",
    "vals_ipw = [x[1] for x in smds_ipw_sorted]\n",
    "colors_ipw = ['red' if v > 0.25 else 'orange' if v > 0.10 else 'green' for v in vals_ipw]\n",
    "\n",
    "ax2.barh(vars_ipw, vals_ipw, color=colors_ipw, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(0.10, color='green', linestyle='--', linewidth=2)\n",
    "ax2.axvline(0.25, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('SMD')\n",
    "ax2.set_title('Love Plot - SMD PONDERADO (IPW)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'Love_Plot_SMD.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n[OK] Love Plot guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. MODELOS DE RESULTADO (Gradient Boosting con Cross-Fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODELOS DE RESULTADO CON CROSS-FITTING\n",
    "# ==============================================================================\n",
    "\n",
    "def entrenar_modelos_resultado(df, outcome_var, covariables, n_folds=5):\n",
    "    \"\"\"Entrena modelos de resultado con Cross-Fitting usando GroupKFold\"\"\"\n",
    "    X = df[covariables].values\n",
    "    y_outcome = df[outcome_var].values\n",
    "    D = df['tratamiento'].values\n",
    "    groups = df['codigo_dane'].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    mu1_hat = np.zeros(len(df))\n",
    "    mu0_hat = np.zeros(len(df))\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=min(n_folds, len(np.unique(groups))))\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(gkf.split(X_scaled, y_outcome, groups)):\n",
    "        train_treated = train_idx[D[train_idx] == 1]\n",
    "        train_control = train_idx[D[train_idx] == 0]\n",
    "        \n",
    "        model_1 = GradientBoostingRegressor(\n",
    "            n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "            min_samples_leaf=5, random_state=CONFIG['RANDOM_STATE']\n",
    "        )\n",
    "        model_0 = GradientBoostingRegressor(\n",
    "            n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "            min_samples_leaf=5, random_state=CONFIG['RANDOM_STATE']\n",
    "        )\n",
    "        \n",
    "        if len(train_treated) >= 5:\n",
    "            model_1.fit(X_scaled[train_treated], y_outcome[train_treated])\n",
    "            mu1_hat[test_idx] = model_1.predict(X_scaled[test_idx])\n",
    "        else:\n",
    "            mu1_hat[test_idx] = y_outcome[D == 1].mean()\n",
    "        \n",
    "        if len(train_control) >= 5:\n",
    "            model_0.fit(X_scaled[train_control], y_outcome[train_control])\n",
    "            mu0_hat[test_idx] = model_0.predict(X_scaled[test_idx])\n",
    "        else:\n",
    "            mu0_hat[test_idx] = y_outcome[D == 0].mean()\n",
    "    \n",
    "    return mu1_hat, mu0_hat\n",
    "\n",
    "print(\"Función entrenar_modelos_resultado() definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. ESTIMADOR AIPW Y WILD CLUSTER BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ESTIMADOR AIPW Y WILD CLUSTER BOOTSTRAP\n",
    "# ==============================================================================\n",
    "\n",
    "def calcular_aipw(Y, D, mu1_hat, mu0_hat, ps, clip_min=0.01, clip_max=0.99):\n",
    "    \"\"\"Calcula el estimador AIPW\"\"\"\n",
    "    ps_clipped = np.clip(ps, clip_min, clip_max)\n",
    "    diff_predicha = mu1_hat - mu0_hat\n",
    "    corr_tratados = D * (Y - mu1_hat) / ps_clipped\n",
    "    corr_controles = (1 - D) * (Y - mu0_hat) / (1 - ps_clipped)\n",
    "    tau_i = diff_predicha + corr_tratados - corr_controles\n",
    "    ate = np.mean(tau_i)\n",
    "    return ate, tau_i\n",
    "\n",
    "def wild_cluster_bootstrap(df, ate_original, tau_i, n_bootstrap=1000):\n",
    "    \"\"\"Wild Cluster Bootstrap con pesos Rademacher\"\"\"\n",
    "    escuelas = df['codigo_dane'].unique()\n",
    "    n_escuelas = len(escuelas)\n",
    "    escuela_idx = {esc: df['codigo_dane'] == esc for esc in escuelas}\n",
    "    ate_bootstrap = []\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        if (b + 1) % 200 == 0:\n",
    "            print(f\"    Bootstrap {b + 1}/{n_bootstrap}...\")\n",
    "        \n",
    "        rademacher = np.random.choice([1, -1], size=n_escuelas)\n",
    "        tau_wild = np.zeros(len(df))\n",
    "        \n",
    "        for i, esc in enumerate(escuelas):\n",
    "            mask = escuela_idx[esc]\n",
    "            tau_wild[mask] = ate_original + rademacher[i] * (tau_i[mask] - ate_original)\n",
    "        \n",
    "        ate_bootstrap.append(np.mean(tau_wild))\n",
    "    \n",
    "    ate_bootstrap = np.array(ate_bootstrap)\n",
    "    se = np.std(ate_bootstrap)\n",
    "    ic_lower = np.percentile(ate_bootstrap, 2.5)\n",
    "    ic_upper = np.percentile(ate_bootstrap, 97.5)\n",
    "    t_stat = ate_original / se if se > 0 else 0\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))\n",
    "    \n",
    "    return {\n",
    "        'ate': ate_original, 'se': se,\n",
    "        'ic_lower': ic_lower, 'ic_upper': ic_upper,\n",
    "        'p_value': p_value, 't_stat': t_stat,\n",
    "        'bootstrap_dist': ate_bootstrap\n",
    "    }\n",
    "\n",
    "print(\"Funciones calcular_aipw() y wild_cluster_bootstrap() definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. ESTIMACIÓN FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ESTIMACIÓN FINAL AIPW/DR\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTIMACIÓN FINAL AIPW/DR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resultados = {}\n",
    "\n",
    "for outcome_name, outcome_var in [('EGRA Total', 'EGRA_Total'), ('EGMA Total', 'EGMA_Total')]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {outcome_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Diferencia simple\n",
    "    trat_mean = df[df['tratamiento']==1][outcome_var].mean()\n",
    "    ctrl_mean = df[df['tratamiento']==0][outcome_var].mean()\n",
    "    diff_simple = trat_mean - ctrl_mean\n",
    "    print(f\"\\n  Diferencia simple: {diff_simple:.2f}\")\n",
    "    \n",
    "    # Modelos de resultado\n",
    "    print(f\"\\n  Entrenando modelos con Cross-Fitting...\")\n",
    "    mu1, mu0 = entrenar_modelos_resultado(df, outcome_var, COVARIABLES)\n",
    "    \n",
    "    # AIPW\n",
    "    Y = df[outcome_var].values\n",
    "    D = df['tratamiento'].values\n",
    "    ps_vals = df['propensity_score'].values\n",
    "    \n",
    "    ate, tau_i = calcular_aipw(Y, D, mu1, mu0, ps_vals)\n",
    "    print(f\"  ATE AIPW puntual: {ate:.4f}\")\n",
    "    \n",
    "    # Wild Cluster Bootstrap\n",
    "    print(f\"\\n  Wild Cluster Bootstrap (B={CONFIG['N_BOOTSTRAP']})...\")\n",
    "    resultado = wild_cluster_bootstrap(df, ate, tau_i, CONFIG['N_BOOTSTRAP'])\n",
    "    resultados[outcome_name] = resultado\n",
    "    \n",
    "    sig = \"***\" if resultado['p_value'] < 0.01 else \"**\" if resultado['p_value'] < 0.05 else \"*\" if resultado['p_value'] < 0.10 else \"\"\n",
    "    print(f\"\\n  RESULTADO:\")\n",
    "    print(f\"    ATE = {resultado['ate']:.4f} {sig}\")\n",
    "    print(f\"    SE  = {resultado['se']:.4f}\")\n",
    "    print(f\"    IC 95% = [{resultado['ic_lower']:.4f}, {resultado['ic_upper']:.4f}]\")\n",
    "    print(f\"    p-valor = {resultado['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 13.1 ESTIMACIÓN POR SUBPRUEBAS\n\nAdemás de los índices totales (EGRA Total y EGMA Total), estimamos el efecto del tratamiento para cada subprueba individual:\n\n### Subpruebas EGRA (Lectura):\n1. **CSL**: Conocimiento del Sonido de las Letras\n2. **DPSS**: Decodificación de Pseudopalabras Sin Sentido\n3. **LP**: Lectura de Palabras\n4. **CL**: Comprensión Lectora\n\n### Subpruebas EGMA (Matemáticas):\n1. **MissNum**: Números Faltantes\n2. **CompMatOp**: Comparación y Operaciones Matemáticas\n3. **Sumas**: Adición\n4. **Restas**: Sustracción\n5. **MultDiv**: Multiplicación y División\n6. **Problemas**: Resolución de Problemas",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# ESTIMACIÓN DE SUBPRUEBAS EGRA\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ESTIMACIÓN DE SUBPRUEBAS EGRA (LECTURA)\")\nprint(\"=\"*80)\n\n# Definir subpruebas EGRA con nombres legibles\nEGRA_subpruebas = {\n    'CSL_pct_correctas': 'Conocimiento Sonidos Letras (CSL)',\n    'DPSS_pct_correctas': 'Decodificación Pseudopalabras (DPSS)',\n    'LP_pct_correctas': 'Lectura de Palabras (LP)',\n    'CL_pct_correctas': 'Comprensión Lectora (CL)'\n}\n\nresultados_egra_sub = {}\n\nfor var, nombre in EGRA_subpruebas.items():\n    print(f\"\\n  {nombre}...\")\n    \n    # Entrenar modelos\n    mu1, mu0 = entrenar_modelos_resultado(df, var, COVARIABLES)\n    \n    # AIPW\n    Y = df[var].values\n    D = df['tratamiento'].values\n    ps_vals = df['propensity_score'].values\n    \n    ate, tau_i = calcular_aipw(Y, D, mu1, mu0, ps_vals)\n    \n    # Bootstrap (menos iteraciones para subpruebas)\n    resultado = wild_cluster_bootstrap(df, ate, tau_i, n_bootstrap=500)\n    resultados_egra_sub[nombre] = resultado\n    resultados_egra_sub[nombre]['variable'] = var\n    \n    sig = \"***\" if resultado['p_value'] < 0.01 else \"**\" if resultado['p_value'] < 0.05 else \"*\" if resultado['p_value'] < 0.10 else \"\"\n    print(f\"    ATE = {resultado['ate']:.2f} (SE={resultado['se']:.2f}, p={resultado['p_value']:.3f}) {sig}\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"RESUMEN SUBPRUEBAS EGRA:\")\nprint(\"-\"*60)\nprint(f\"{'Subprueba':<45} {'ATE':>8} {'SE':>8} {'p-valor':>10}\")\nprint(\"-\"*60)\nfor nombre, res in resultados_egra_sub.items():\n    sig = \"***\" if res['p_value'] < 0.01 else \"**\" if res['p_value'] < 0.05 else \"*\" if res['p_value'] < 0.10 else \"\"\n    print(f\"{nombre:<45} {res['ate']:>8.2f} {res['se']:>8.2f} {res['p_value']:>10.3f} {sig}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# ESTIMACIÓN DE SUBPRUEBAS EGMA\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ESTIMACIÓN DE SUBPRUEBAS EGMA (MATEMÁTICAS)\")\nprint(\"=\"*80)\n\n# Definir subpruebas EGMA con nombres legibles\nEGMA_subpruebas = {\n    'miss_num_pct_correctas': 'Números Faltantes (MissNum)',\n    'comp_mat_op_pct_correctas': 'Comparación y Operaciones (CompMatOp)',\n    'sums_pct_correctas': 'Sumas',\n    'substract_pct_correctas': 'Restas',\n    'mult_div_pct_correctas': 'Multiplicación/División (MultDiv)',\n    'res_problems_pct_correctas': 'Resolución de Problemas'\n}\n\nresultados_egma_sub = {}\n\nfor var, nombre in EGMA_subpruebas.items():\n    print(f\"\\n  {nombre}...\")\n    \n    # Entrenar modelos\n    mu1, mu0 = entrenar_modelos_resultado(df, var, COVARIABLES)\n    \n    # AIPW\n    Y = df[var].values\n    D = df['tratamiento'].values\n    ps_vals = df['propensity_score'].values\n    \n    ate, tau_i = calcular_aipw(Y, D, mu1, mu0, ps_vals)\n    \n    # Bootstrap (menos iteraciones para subpruebas)\n    resultado = wild_cluster_bootstrap(df, ate, tau_i, n_bootstrap=500)\n    resultados_egma_sub[nombre] = resultado\n    resultados_egma_sub[nombre]['variable'] = var\n    \n    sig = \"***\" if resultado['p_value'] < 0.01 else \"**\" if resultado['p_value'] < 0.05 else \"*\" if resultado['p_value'] < 0.10 else \"\"\n    print(f\"    ATE = {resultado['ate']:.2f} (SE={resultado['se']:.2f}, p={resultado['p_value']:.3f}) {sig}\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"RESUMEN SUBPRUEBAS EGMA:\")\nprint(\"-\"*60)\nprint(f\"{'Subprueba':<45} {'ATE':>8} {'SE':>8} {'p-valor':>10}\")\nprint(\"-\"*60)\nfor nombre, res in resultados_egma_sub.items():\n    sig = \"***\" if res['p_value'] < 0.01 else \"**\" if res['p_value'] < 0.05 else \"*\" if res['p_value'] < 0.10 else \"\"\n    print(f\"{nombre:<45} {res['ate']:>8.2f} {res['se']:>8.2f} {res['p_value']:>10.3f} {sig}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# EXPORTAR RESULTADOS DE SUBPRUEBAS A CSV Y EXCEL\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORTANDO RESULTADOS DE SUBPRUEBAS\")\nprint(\"=\"*80)\n\n# Combinar todos los resultados de subpruebas\ntodas_subpruebas = []\n\n# EGRA\nfor nombre, res in resultados_egra_sub.items():\n    todas_subpruebas.append({\n        'ATE': res['ate'],\n        'SE': res['se'],\n        'IC_Lower': res['ic_lower'],\n        'IC_Upper': res['ic_upper'],\n        'p_value': res['p_value'],\n        't_stat': res['t_stat'],\n        'n': len(df),\n        'n_trat': (df['tratamiento']==1).sum(),\n        'n_ctrl': (df['tratamiento']==0).sum(),\n        'Variable': res['variable'],\n        'Nombre': nombre,\n        'Tipo': 'EGRA',\n        'Significativo_05': res['p_value'] < 0.05,\n        'Significativo_10': res['p_value'] < 0.10\n    })\n\n# EGMA\nfor nombre, res in resultados_egma_sub.items():\n    todas_subpruebas.append({\n        'ATE': res['ate'],\n        'SE': res['se'],\n        'IC_Lower': res['ic_lower'],\n        'IC_Upper': res['ic_upper'],\n        'p_value': res['p_value'],\n        't_stat': res['t_stat'],\n        'n': len(df),\n        'n_trat': (df['tratamiento']==1).sum(),\n        'n_ctrl': (df['tratamiento']==0).sum(),\n        'Variable': res['variable'],\n        'Nombre': nombre,\n        'Tipo': 'EGMA',\n        'Significativo_05': res['p_value'] < 0.05,\n        'Significativo_10': res['p_value'] < 0.10\n    })\n\n# Agregar totales\nfor nombre, res in resultados.items():\n    tipo = 'TOTAL'\n    var = 'EGRA_Total' if 'EGRA' in nombre else 'EGMA_Total'\n    todas_subpruebas.append({\n        'ATE': res['ate'],\n        'SE': res['se'],\n        'IC_Lower': res['ic_lower'],\n        'IC_Upper': res['ic_upper'],\n        'p_value': res['p_value'],\n        't_stat': res['t_stat'],\n        'n': len(df),\n        'n_trat': (df['tratamiento']==1).sum(),\n        'n_ctrl': (df['tratamiento']==0).sum(),\n        'Variable': var,\n        'Nombre': nombre.replace(' Total', ' Total (Lectura)') if 'EGRA' in nombre else nombre.replace(' Total', ' Total (Matematicas)'),\n        'Tipo': tipo,\n        'Significativo_05': res['p_value'] < 0.05,\n        'Significativo_10': res['p_value'] < 0.10\n    })\n\n# Crear DataFrame y exportar\ndf_subpruebas = pd.DataFrame(todas_subpruebas)\ndf_subpruebas.to_csv(OUTPUT_DIR / 'resultados_subpruebas.csv', index=False, encoding='utf-8-sig')\ndf_subpruebas.to_excel(OUTPUT_DIR / 'resultados_subpruebas.xlsx', index=False)\n\nprint(f\"\\n[OK] Archivos exportados:\")\nprint(f\"  - {OUTPUT_DIR / 'resultados_subpruebas.csv'}\")\nprint(f\"  - {OUTPUT_DIR / 'resultados_subpruebas.xlsx'}\")\n\nprint(f\"\\n\" + \"-\"*80)\nprint(\"TABLA COMPLETA DE RESULTADOS POR SUBPRUEBA:\")\nprint(\"-\"*80)\ndisplay(df_subpruebas[['Nombre', 'Tipo', 'ATE', 'SE', 'IC_Lower', 'IC_Upper', 'p_value', 'Significativo_05']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ==============================================================================\n# FOREST PLOT DE SUBPRUEBAS EGRA Y EGMA\n# ==============================================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 10))\n\n# ============ FOREST PLOT EGRA ============\nax1 = axes[0]\negra_nombres = list(resultados_egra_sub.keys()) + ['EGRA Total']\negra_ates = [res['ate'] for res in resultados_egra_sub.values()] + [resultados['EGRA Total']['ate']]\negra_lowers = [res['ic_lower'] for res in resultados_egra_sub.values()] + [resultados['EGRA Total']['ic_lower']]\negra_uppers = [res['ic_upper'] for res in resultados_egra_sub.values()] + [resultados['EGRA Total']['ic_upper']]\negra_pvals = [res['p_value'] for res in resultados_egra_sub.values()] + [resultados['EGRA Total']['p_value']]\n\ny_pos = np.arange(len(egra_nombres))\ncolors_egra = ['steelblue' if p < 0.10 else 'gray' for p in egra_pvals]\ncolors_egra[-1] = 'darkblue'  # Total en color diferente\n\nfor i, (ate, lower, upper, color) in enumerate(zip(egra_ates, egra_lowers, egra_uppers, colors_egra)):\n    ax1.errorbar(ate, i, xerr=[[ate-lower], [upper-ate]], fmt='o', color=color, \n                 capsize=5, capthick=2, markersize=10, markeredgecolor='black', linewidth=2)\n\nax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Sin efecto')\nax1.set_yticks(y_pos)\nax1.set_yticklabels(egra_nombres, fontsize=10)\nax1.set_xlabel('ATE (puntos porcentuales)', fontsize=11)\nax1.set_title('Forest Plot - Subpruebas EGRA (Lectura)\\nIC 95% Wild Cluster Bootstrap', fontweight='bold', fontsize=12)\nax1.grid(True, alpha=0.3, axis='x')\nax1.legend(loc='lower right')\n\n# Agregar p-valores\nfor i, (ate, p) in enumerate(zip(egra_ates, egra_pvals)):\n    sig = \"***\" if p < 0.01 else \"**\" if p < 0.05 else \"*\" if p < 0.10 else \"\"\n    ax1.annotate(f'p={p:.3f}{sig}', xy=(ate, i), xytext=(5, 0), \n                 textcoords='offset points', fontsize=9, va='center')\n\n# ============ FOREST PLOT EGMA ============\nax2 = axes[1]\negma_nombres = list(resultados_egma_sub.keys()) + ['EGMA Total']\negma_ates = [res['ate'] for res in resultados_egma_sub.values()] + [resultados['EGMA Total']['ate']]\negma_lowers = [res['ic_lower'] for res in resultados_egma_sub.values()] + [resultados['EGMA Total']['ic_lower']]\negma_uppers = [res['ic_upper'] for res in resultados_egma_sub.values()] + [resultados['EGMA Total']['ic_upper']]\negma_pvals = [res['p_value'] for res in resultados_egma_sub.values()] + [resultados['EGMA Total']['p_value']]\n\ny_pos = np.arange(len(egma_nombres))\ncolors_egma = ['forestgreen' if p < 0.10 else 'gray' for p in egma_pvals]\ncolors_egma[-1] = 'darkgreen'  # Total en color diferente\n\nfor i, (ate, lower, upper, color) in enumerate(zip(egma_ates, egma_lowers, egma_uppers, colors_egma)):\n    ax2.errorbar(ate, i, xerr=[[ate-lower], [upper-ate]], fmt='s', color=color,\n                 capsize=5, capthick=2, markersize=10, markeredgecolor='black', linewidth=2)\n\nax2.axvline(0, color='red', linestyle='--', linewidth=2, label='Sin efecto')\nax2.set_yticks(y_pos)\nax2.set_yticklabels(egma_nombres, fontsize=10)\nax2.set_xlabel('ATE (puntos porcentuales)', fontsize=11)\nax2.set_title('Forest Plot - Subpruebas EGMA (Matemáticas)\\nIC 95% Wild Cluster Bootstrap', fontweight='bold', fontsize=12)\nax2.grid(True, alpha=0.3, axis='x')\nax2.legend(loc='lower right')\n\n# Agregar p-valores\nfor i, (ate, p) in enumerate(zip(egma_ates, egma_pvals)):\n    sig = \"***\" if p < 0.01 else \"**\" if p < 0.05 else \"*\" if p < 0.10 else \"\"\n    ax2.annotate(f'p={p:.3f}{sig}', xy=(ate, i), xytext=(5, 0),\n                 textcoords='offset points', fontsize=9, va='center')\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'Forest_Plot_Subpruebas.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"\\n[OK] Forest_Plot_Subpruebas.png guardado en {OUTPUT_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. TABLA RESUMEN DE RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLA RESUMEN\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TABLA RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\n{'Outcome':<15} {'ATE':>12} {'SE':>10} {'IC 95%':>28} {'p-valor':>12} {'Sig':>6}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for nombre, res in resultados.items():\n",
    "    sig = \"***\" if res['p_value'] < 0.01 else \"**\" if res['p_value'] < 0.05 else \"*\" if res['p_value'] < 0.10 else \"\"\n",
    "    ic = f\"[{res['ic_lower']:.2f}, {res['ic_upper']:.2f}]\"\n",
    "    print(f\"{nombre:<15} {res['ate']:>12.4f} {res['se']:>10.4f} {ic:>28} {res['p_value']:>12.4f} {sig:>6}\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"Significancia: *** p<0.01, ** p<0.05, * p<0.10\")\n",
    "print(f\"Clusters: {df['codigo_dane'].nunique()} escuelas | N: {len(df)} estudiantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. VISUALIZACIONES FINALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURA 1: ANÁLISIS EXPLORATORIO\n",
    "# ==============================================================================\n",
    "\n",
    "fig1, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (outcome, ax_row) in enumerate(zip(['EGRA_Total', 'EGMA_Total'], [0, 1])):\n",
    "    # Histogramas\n",
    "    ax = axes[ax_row, 0]\n",
    "    for trat, color, label in [(1, 'steelblue', 'Tratamiento'), (0, 'coral', 'Control')]:\n",
    "        data = df[df['tratamiento'] == trat][outcome]\n",
    "        ax.hist(data, bins=30, alpha=0.6, color=color, label=f'{label} (n={len(data)})', density=True)\n",
    "    ax.set_xlabel(f'Puntaje {outcome.split(\"_\")[0]}')\n",
    "    ax.set_ylabel('Densidad')\n",
    "    ax.set_title(f'Distribución {outcome.split(\"_\")[0]} por Grupo', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot por escuela\n",
    "    ax2 = axes[ax_row, 1]\n",
    "    escuelas_orden = df.groupby('codigo_dane')[outcome].median().sort_values().index\n",
    "    colores = ['steelblue' if df[df['codigo_dane']==e]['tratamiento'].iloc[0]==1 else 'coral' for e in escuelas_orden]\n",
    "    bp = ax2.boxplot([df[df['codigo_dane']==e][outcome].values for e in escuelas_orden],\n",
    "                      positions=range(len(escuelas_orden)), patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colores):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    ax2.set_xlabel('Escuelas')\n",
    "    ax2.set_ylabel(f'Puntaje {outcome.split(\"_\")[0]}')\n",
    "    ax2.set_title(f'{outcome.split(\"_\")[0]} por Escuela (Azul=Trat, Rojo=Ctrl)', fontweight='bold')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'Analisis_Exploratorio.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] Analisis_Exploratorio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURA 2: DIAGNÓSTICOS DEL MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ROC\n",
    "ax1 = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(y, ps)\n",
    "ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auroc:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax1.fill_between(fpr, tpr, alpha=0.3)\n",
    "ax1.set_xlabel('FPR')\n",
    "ax1.set_ylabel('TPR')\n",
    "ax1.set_title('Curva ROC', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PS distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(ps[y==1], bins=20, alpha=0.6, label='Tratamiento', density=True, color='steelblue')\n",
    "ax2.hist(ps[y==0], bins=20, alpha=0.6, label='Control', density=True, color='coral')\n",
    "ax2.set_xlabel('Propensity Score')\n",
    "ax2.set_ylabel('Densidad')\n",
    "ax2.set_title('Distribución de Propensity Scores', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Love Plot\n",
    "ax3 = axes[1, 0]\n",
    "smds_sorted = sorted(smds_crudo.items(), key=lambda x: x[1], reverse=True)\n",
    "vars_s = [x[0] for x in smds_sorted]\n",
    "vals_s = [x[1] for x in smds_sorted]\n",
    "colors_s = ['red' if v > 0.25 else 'orange' if v > 0.10 else 'green' for v in vals_s]\n",
    "ax3.barh(vars_s, vals_s, color=colors_s, alpha=0.7)\n",
    "ax3.axvline(0.10, color='green', linestyle='--', linewidth=2)\n",
    "ax3.axvline(0.25, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('SMD')\n",
    "ax3.set_title('Balance de Covariables', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# PS boxplot\n",
    "ax4 = axes[1, 1]\n",
    "bp = ax4.boxplot([ps[y==0], ps[y==1]], labels=['Control', 'Tratamiento'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('coral')\n",
    "bp['boxes'][1].set_facecolor('steelblue')\n",
    "ax4.set_ylabel('Propensity Score')\n",
    "ax4.set_title('PS por Grupo', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'Diagnosticos_modelo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] Diagnosticos_modelo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURA 3: RESULTADOS FINALES\n",
    "# ==============================================================================\n",
    "\n",
    "fig3, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Forest Plot\n",
    "ax1 = axes[0, 0]\n",
    "nombres = list(resultados.keys())\n",
    "ates = [r['ate'] for r in resultados.values()]\n",
    "lowers = [r['ic_lower'] for r in resultados.values()]\n",
    "uppers = [r['ic_upper'] for r in resultados.values()]\n",
    "y_pos = np.arange(len(nombres))\n",
    "errors = [[ate - l for ate, l in zip(ates, lowers)], [u - ate for ate, u in zip(ates, uppers)]]\n",
    "colors = ['gray' for _ in nombres]\n",
    "ax1.barh(y_pos, ates, xerr=errors, color=colors, alpha=0.7, capsize=5, edgecolor='black')\n",
    "ax1.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(nombres)\n",
    "ax1.set_xlabel('ATE (puntos)')\n",
    "ax1.set_title('Forest Plot - Efectos del Tratamiento (IC 95%)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Bootstrap EGRA\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(resultados['EGRA Total']['bootstrap_dist'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax2.axvline(resultados['EGRA Total']['ate'], color='red', linestyle='-', linewidth=2, label='ATE')\n",
    "ax2.axvline(0, color='black', linestyle='--', linewidth=1.5, label='Efecto nulo')\n",
    "ax2.set_xlabel('ATE')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Bootstrap - EGRA Total', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bootstrap EGMA\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(resultados['EGMA Total']['bootstrap_dist'], bins=50, alpha=0.7, color='forestgreen', edgecolor='black')\n",
    "ax3.axvline(resultados['EGMA Total']['ate'], color='red', linestyle='-', linewidth=2, label='ATE')\n",
    "ax3.axvline(0, color='black', linestyle='--', linewidth=1.5, label='Efecto nulo')\n",
    "ax3.set_xlabel('ATE')\n",
    "ax3.set_ylabel('Frecuencia')\n",
    "ax3.set_title('Bootstrap - EGMA Total', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparación\n",
    "ax4 = axes[1, 1]\n",
    "diff_simples = [df[df['tratamiento']==1][o].mean() - df[df['tratamiento']==0][o].mean() for o in ['EGRA_Total', 'EGMA_Total']]\n",
    "aipw_ates = [r['ate'] for r in resultados.values()]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, diff_simples, width, label='Diferencia Simple', color='lightblue', edgecolor='black')\n",
    "ax4.bar(x + width/2, aipw_ates, width, label='AIPW/DR', color='steelblue', edgecolor='black')\n",
    "ax4.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(['EGRA', 'EGMA'])\n",
    "ax4.set_ylabel('Efecto')\n",
    "ax4.set_title('Diferencia Simple vs AIPW/DR', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'Resultados_finales.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"[OK] Resultados_finales.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. GUARDAR RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GUARDAR RESULTADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDANDO RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resultados_df = pd.DataFrame([\n",
    "    {\n",
    "        'Outcome': nombre,\n",
    "        'ATE': res['ate'],\n",
    "        'SE': res['se'],\n",
    "        'IC_Lower': res['ic_lower'],\n",
    "        'IC_Upper': res['ic_upper'],\n",
    "        'p_value': res['p_value'],\n",
    "        't_stat': res['t_stat'],\n",
    "        'Significativo_05': res['p_value'] < 0.05,\n",
    "        'Significativo_10': res['p_value'] < 0.10\n",
    "    }\n",
    "    for nombre, res in resultados.items()\n",
    "])\n",
    "\n",
    "resultados_df.to_csv(OUTPUT_DIR / 'resultados_finales.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n[OK] resultados_finales.csv\")\n",
    "display(resultados_df)\n",
    "\n",
    "diagnosticos = {\n",
    "    'AUROC': auroc,\n",
    "    'SMD_promedio': smd_promedio_crudo,\n",
    "    'ESS_tratamiento_pct': ess_t / n_tratados * 100,\n",
    "    'ESS_control_pct': ess_c / n_control * 100,\n",
    "    'N_estudiantes': len(df),\n",
    "    'N_escuelas': df['codigo_dane'].nunique(),\n",
    "    'N_bootstrap': CONFIG['N_BOOTSTRAP']\n",
    "}\n",
    "\n",
    "pd.DataFrame([diagnosticos]).to_csv(OUTPUT_DIR / 'diagnosticos.csv', index=False)\n",
    "print(f\"[OK] diagnosticos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. CONCLUSIONES FINALES\n",
    "\n",
    "---\n",
    "\n",
    "## Hallazgo Principal\n",
    "\n",
    "**No se encontraron efectos estadísticamente significativos del programa PTA/FI 3.0 sobre los resultados de EGRA (lectura) ni EGMA (matemáticas).**\n",
    "\n",
    "| Outcome | ATE | IC 95% | p-valor | Significativo |\n",
    "|---------|-----|--------|---------|---------------|\n",
    "| EGRA Total | -1.41 | [-6.84, 3.72] | 0.601 | No |\n",
    "| EGMA Total | -1.74 | [-6.41, 2.83] | 0.467 | No |\n",
    "\n",
    "---\n",
    "\n",
    "## Aspectos Metodológicos\n",
    "\n",
    "- **Estimador AIPW/DR**: Doblemente robusto\n",
    "- **Wild Cluster Bootstrap (B=1000)**: Inferencia válida con pocos clusters\n",
    "- **Cross-Fitting con GroupKFold**: Evita sobreajuste\n",
    "- **Gradient Boosting**: Captura relaciones no lineales\n",
    "\n",
    "## Diagnósticos\n",
    "\n",
    "- **AUROC**: 0.803 (rango óptimo)\n",
    "- **ESS**: >85% (poca pérdida de información)\n",
    "- **N**: 770 estudiantes, 22 escuelas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "RESUMEN DE LA EVALUACIÓN DE IMPACTO PTA/FI 3.0\n",
    "{'='*60}\n",
    "\n",
    "MUESTRA:\n",
    "  - Estudiantes: {len(df):,}\n",
    "  - Escuelas: {df['codigo_dane'].nunique()}\n",
    "\n",
    "DIAGNÓSTICOS:\n",
    "  - AUROC: {auroc:.3f}\n",
    "  - SMD Promedio: {smd_promedio_crudo:.3f}\n",
    "\n",
    "RESULTADOS:\n",
    "  EGRA Total: ATE = {resultados['EGRA Total']['ate']:.2f} (p={resultados['EGRA Total']['p_value']:.3f})\n",
    "  EGMA Total: ATE = {resultados['EGMA Total']['ate']:.2f} (p={resultados['EGMA Total']['p_value']:.3f})\n",
    "\n",
    "CONCLUSIÓN:\n",
    "  No se encontraron efectos estadísticamente significativos.\n",
    "\n",
    "{'='*60}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}